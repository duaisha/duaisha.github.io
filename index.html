<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-V3LHXHHFY5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-V3LHXHHFY5');
</script>
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Isha Dua</title>

  <meta name="author" content="Isha Dua">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <style>
    ul {
      list-style: none;      /*removes the default bullet*/
      margin-left: 3%;       /*gives the bullet a small left margin*/
      padding-left: 1.2em;   /*sets the list text to the right*/
      text-indent: -1.2em;   /*indents the list text*/
  }
    li:before {                /*puts the Unicode before the text*/
      content: "ðŸŒŸ";      /*designate the Unicode character's number*/
      font-size: large;       /*control the bullet size*/
      color: DarkOrange;         /*control the symbol color*/
      margin-right: .5em;     /*give a small margin to the Unicode*/
  }
   li {
      padding: .2em;          /*space between the Unicode and the text*/
  }
  </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Isha Dua</name>
              </p>
              <p>
                Hi, I am a Machine Learning Researcher in Mercedes Benz, Research and Developement, India. My research interest lies at the intersection of
                Computer Vision, Machine Learning and Human computer interfaces. I am currently working on utilization of synthetic dataset for solving
                image processing/complex machine learning task in real world. I develop algorithms that focus on Supervised/Unsupervised/Semi-Supervised Domain Adaptation.
                <br>
                <br>
                Before joining Mercedes Benz, I was a Research Scholar in <a href="https://cvit.iiit.ac.in/" target="_blank"> Computer Vision Lab, IIIT Hyderabad</a>,
                where I was fortunate to be advised by <a href="https://faculty.iiit.ac.in/~jawahar/" target="_blank"> Professor C.V.Jawahar</a>.
                Here, I worked on <a href="https://www.microsoft.com/en-us/research/project/hams/" target="_blank">HAMS: Harnessing AutoMobiles
                for Safety</a> with Principal Researcher <a href="https://www.microsoft.com/en-us/research/people/padmanab/" target="_blank">Venkat Padmanabhan</a>
                and Postdoctoral Researcher <a href="https://www.microsoft.com/en-us/research/people/t-snaksh/" target="_blank">Akshay Uttama Nambi</a>
                from <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-india/" target="_blank">Microsoft
                Research Lab, India.</a>
                <br>
                <br>
                Besides this I also worked on computer vision for wild life prevention with <a href="http://geu.ac.in/research/home.html"
             	  target="_blank">Professor Ankush Mittal</a> and <a href="https://scholar.google.co.in/citations?user=QU2O6JMAAAAJ&hl=en"
                target="_blank">Professor Balasubramanian Raman.</a>
                <br><br>
              <p style="text-align:center">
                <a href="mailto:isha.dua@mercedes-benz.com">Email</a> &nbsp/&nbsp
                <a href="data/Resume_isha_2025.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.in/citations?user=c76ke9kAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/duaisha">Github</a>&nbsp/&nbsp
                <br><br>
                <a href="https://twitter.com/isharadz">Twitter</a>&nbsp/&nbsp
                <a href="https://www.linkedin.com/in/isha-dua-115484b7/">LinkedIn</a>&nbsp/&nbsp
                <a href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/Thesis/MS/isha_dua/Isha_Thesis_Driver_Attention.pdf">Master's Thesis</a>&nbsp
              </p>
            </td>
            <td style="width:100%;max-width:100%">
              <a href="./data/profile_photo2.jpg"><img style="border-radius:100%;width:80%;height:37%;max-width:100%" alt="profile photo" src="data/profile_photo2.jpg" class="hoverZoomLink"></a>
              <!-- <a href="data/myAvatar.png"><img style="border-radius:48%;width:100%;max-width:100" alt="profile photo" src="data/myAvatar-2.png" class="hoverZoomLink"></a> -->
            </td>
          </tr>
        </tbody></table>

<!--
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>News</heading>
            <ul>
            <li> <mark>(August 2022):</mark> Our work on <a href="https://arxiv.org/pdf/2207.13083.pdf">TAPUDD: Task Agnostic and Post-hoc Unseen Distribution Detection</a>  is also accepted as a short paper to <a href="https://eccv22-arow.github.io">AROW Workshop</a>, ECCV 2022.
            <li> <mark>(August 2022):</mark> Our work on <a href="https://arxiv.org/pdf/2207.13083.pdf">TAPUDD: Task Agnostic and Post-hoc Unseen Distribution Detection</a> is accepted to <a href="https://wacv2023.thecvf.com/home">WACV 2023</a>.
            <li> <mark>(August 2022):</mark> Our work on <a href="https://arxiv.org/pdf/2208.13376.pdf">Reweighting Strategy based on Synthetic Data Identification for Sentence Similarity</a> is accepted to <a href="https://coling2022.org">COLING 2022</a>.
            <li> <mark>(July 2022):</mark> Our work on <a href="https://arxiv.org/pdf/2208.08853.pdf">Automatic Detection of Noisy Electrocardiogram Signals without Explicit Noise Labels</a> is accepted to <a href="https://sites.google.com/view/icpr-prha2022/home?authuser=0">PRHA Workshop</a>, ICPR 2022.
            <li> <mark>(May 2022):</mark> Serving as a reviewer for <a href="https://eccv2022.ecva.net">ECCV 2022.
            <li> <mark>(Mar 2022):</mark> Our work on <a href="https://arxiv.org/pdf/2201.07788.pdf">ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes</a> is accepted to CVPR 2022.
            <li> <mark>(Apr 2021):</mark> Our work on <a href="https://openaccess.thecvf.com/content/CVPR2021W/MULA/papers/Dua_Beyond_VQA_Generating_Multi-Word_Answers_and_Rationales_to_Visual_Questions_CVPRW_2021_paper.pdf">Beyond VQA: Generating Multi-word Answers and Rationales to Visual Questions</a> is accepted to <a href="https://mula-workshop.github.io/">MULA workshop, CVPR 2021</a>.
            <li><mark>(Nov 2020):</mark> Serving as a Volunteer and Poster Mentor at Women in Machine Learning(WiML) workshop at NeurIPS 2020.</li>
            <li> <mark>(Sep 2020):</mark> I started my M.S. in the Graduate School of AI at <a href="https://www.kaist.ac.kr/en/">KAIST</a>.
            <li> <mark>(Jun 2020):</mark> Serving as a volunteer for <a href="https://icml.cc/Conferences/2020">ICML 2020</a> and <a href="https://acl2020.org/">ACL 2020</a></li>
            <li> <mark>(Jun 2020):</mark> I started summer internship at <a href="https://www.brown.edu/">Brown University</a> under the supervision of <a href="http://srinathsridhar.com/">Prof. Srinath Sridhar</a>.
            <li> <mark>(Mar 2020):</mark> Serving as a Scholarship Application Reviewer</highlight> for the 2020 Grace Hopper Celebration (GHC 2020)</li>
            <li> <mark>(Aug 2019):</mark> Our paper <a href="https://ieeexplore.ieee.org/document/8969343">VayuAnukulani: Adaptive Memory Networks for Air Pollution Forecasting</a> got accepted at <a href="http://www.2019.ieeeglobalsip.org/2019.ieeeglobalsip.org/index.html">GlobalSIP 2019</a>. Code is available on GitHub.</li>
            <li> <mark>(Jul 2019):</mark> Attended Summer School on Computer Vision 2019, CVIT, IIIT Hyderabad</li>
            <li> <mark>(May 2019):</mark> <a href="https://economictimes.indiatimes.com/etcampusstars/pasteditions?edition=2019">ET Campus Stars</a> by economic times, India's brightest engineers (2018-2019)</li>
            <li> <mark>(Nov 2018):</mark> Our team won second prize in <a href="http://www.celestiniprojectindia.com/">Celestini Project India</a> sponsored by Marconi Society and Google [News Coverage:<a href="https://www.financialexpress.com/industry/technology/delhi-pollution-indian-students-build-air-quality-measuring-app-air-congnizer/1373275/">Financial Express</a>

                    <a href="https://www.hindustantimes.com/education/indian-students-bag-us-award-for-developing-innovative-app-to-check-air-quality-index-levels/story-E6LrYSXEpEcyja7MJRh2TM.html">Hindustan Times</a>
              /
                    <a href="https://gadgets.ndtv.com/apps/news/delhi-air-pollution-college-students-develop-app-to-measure-air-quality-1943152">NDTV</a>
              /
                    <a href="https://www.business-standard.com/article/technology/indian-students-win-us-award-for-developing-mobile-app-to-monitor-aqi-level-118110500737_1.html">Business Standard</a>
              /
                    <a href="https://www.firstpost.com/tech/science/app-to-measures-pollution-levels-created-by-indian-students-wins-global-award-5505721.html">First Post</a>
              /
                    <a href="https://www.indiatoday.in/education-today/gk-current-affairs/story/air-quality-diwali-delhi-government-measures-1383399-2018-11-06">India Today</a>
              /
                    <a href="https://www.hindustantimes.com/education/indian-students-bag-us-award-for-developing-innovative-app-to-check-air-quality-index-levels/story-E6LrYSXEpEcyja7MJRh2TM.html">Hindustan Times</a>
              ]</li>
            <li> <mark>(Sep 2018):</mark> Received Grace Hopper Celebration India (GHCI) Student Scholarship </li>
            <li> <mark>(Nov 2017):</mark> Our team won third prize in Hack Infinity </li>
            <li> <mark>(Sep 2017):</mark> Our team won sixth position in India Hacks by Hackerearth</li>
            <li> <mark>(Mar 2017):</mark> Our team won second prize in Hack In The North</li>
            </ul>
          </td></tr>
        </tbody></table> -->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <heading>Awards and Achievements</heading>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <table cellpadding="20" border="0" width="100%" align="center"><tbody> -->
          <tr>
            <!-- <td style="padding:10px;width:100%;vertical-align:middle"> -->
              <p><font color="DodgerBlue">Patent Award 2022</font></p>
              <p> I was rewarded an award in "Patent Rewards Category" during innovation week 2022 at Mercedes Benz for our filed patent on
                "System for improving keypoints location and method thereof" </p>
              <!-- </td> -->
           </tr>

           <tr>
             <!-- <td style="padding:10px;width:100%;vertical-align:middle"> -->
               <p><font color="DodgerBlue">Process Innovation Award 2022</font></p>
               <p> I was rewarded an award in "Process Innovation Category" during innovation week 2022 at Mercedes Benz for our implemented
               innovation on "Multiperson Human Pose Estiamtion using synthetic dataset" </p>
               <!-- </td> -->
            </tr>

            <tr>
              <!-- <td style="padding:10px;width:100%;vertical-align:middle"> -->
                <p><font color="DodgerBlue">Process Innovation Award 2021</font></p>
                <p> I was rewarded an award in "Process Innovation Category" during innovation week 2021 at Mercedes Benz for our implemented
                innovation on "Accelerated deep learning framework for efficient and faster training" </p>
                <!-- </td> -->
             </tr>

             <tr>
               <!-- <td style="padding:10px;width:100%;vertical-align:middle"> -->
                 <p><font color="DodgerBlue">Silver Star Award 2021</font></p>
                 <p> I was awarded with silver star award from my manager <a href="https://www.linkedin.com/in/brijeshpillai/" target="_blank">
                   Brijesh Pillai</a> for my contribution in developing the accelerated
                  deep learning library and for the work on bridging domain gap between synthetic and real domains</p>
                 <!-- </td> -->
              </tr>

            <tr>
              <!-- <td style="padding:10px;width:100%;vertical-align:middle"> -->
                <p><font color="DodgerBlue">First prize at Google Hackthon 2018</font></p>
                <p> I won the first prize in the ML track at Google Hackthon 2018.
                    Here I worked on waste segregation problem using Machine Learning and Deep Learning. </p>
                <!-- </td> -->
             </tr>

             <tr>
               <!-- <td style="padding:10px;width:100%;vertical-align:middle"> -->
                 <p><font color="DodgerBlue">Outstanding mentor award at Foundation of AIML course 2018</font></p>
                 <p> This award was given to me for assisting in teaching machine learning to industry folks.
                     The course was taken by Prof C.V.Jawahar and Prof Anoop Namboodiri.
                </p>
                 <!-- </td> -->
              </tr>

              <tr>
                <!-- <td style="padding:10px;width:100%;vertical-align:middle"> -->
                  <p><font color="DodgerBlue">First prize at Microsoft.code.fun.do 2018</font></p>
                  <p> Microsoft code fun do is hackthon organized by Microsoft and focus on building applications useful for people.
                     I contributing by building game which can be be played using eyes as input.
                     Other uses include provided basic use of technology for paralized people using eyes and computer screen,
                     Medical Research and extra feedback through eyes for E-commerece field.
                <!-- </td> -->
               </tr>

               <tr>
                 <!-- <td style="padding:10px;width:100%;vertical-align:middle"> -->
                   <p><font color="DodgerBlue">Sixth prize at India Hacks 2017</font></p>
                   <p> This was hackthon organized by Hacker Earth. Here I worked estimating the breathing rate
                      and pulse rate using AI and Machine Learning.
                  </p>
                   <!-- </td> -->
                </tr>

                <tr>
                  <!-- <td style="padding:10px;width:100%;vertical-align:middle"> -->
                    <p><font color="DodgerBlue">Other Achievement includes</font></p>
                    <ul>
                   		<li><b>First position</b> in college, BTech First Year</li>
                   		<li><b>Third position</b> in CSE department, BTech Third year</li>
                   		<li><b>Second position</b> in CSE department, Btech Forth Year</li>
                   </ul>
                    <!-- </td> -->
                 </tr>

              </tbody></table>


              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <tr>
                  <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Publications</heading>
                  </td>
                </tr>
              </tbody></table>
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <!-- <table cellpadding="20" border="0" width="100%" align="center"><tbody> -->
              <tr>
                <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/ECCV.jpeg" width="100%"></td>
                <td width="100%" valign="center">
                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="data/vayu.png" alt="blind-date" width="280" height="250"></td> -->
                <!-- <td style="width:40%;vertical-align:middle"><img src="data/vayu.png" width="100%"></td>
                <td width="100%" valign="center"> -->
                <!-- <td width="75%" valign="middle"> -->
                        <papertitle>Semi-Supervised Domain Adaptation by Similarity based Pseudo-label Injection.</papertitle>
                        <br>
                        Abhay Rawat, <strong>Isha Dua</strong>, Saurav Gupta and Rahul Tallamraju
                        <br>
                        Accepted at <em>ECCV, 2022</em> Workshop
                        <br>
                          <a href="https://deepai.org/publication/semi-supervised-domain-adaptation-by-similarity-based-pseudo-label-injection">Paper</a>
                          /
                          <a href="https://github.com/abhayraw1/spi">Code</a>
                       <p>To align the two domains, we leverage contrastive losses to learn a semantically meaningful and
                          a domain agnostic feature space using the supervised samples from both domains. We further use the pseudo-label
                          approach to gradually inject the unlabeled target samples into training by comparing
                          their feature representation to those of the labeled samples from both the source and target domains. </p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/ETL.png" width="100%"></td>
                    <td width="100%" valign="center">
                    <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="data/vayu.png" alt="blind-date" width="280" height="250"></td> -->
                    <!-- <td style="width:40%;vertical-align:middle"><img src="data/vayu.png" width="100%"></td>
                    <td width="100%" valign="center"> -->
                    <!-- <td width="75%" valign="middle"> -->
                            <papertitle>ETL: Efficient Transfer Learning for Face Tasks.</papertitle>
                            <br>
                            Thrupthi Ann John, <strong>Isha Dua</strong>, Vineeth N Balasubramanian, C. V. Jawahar
                            <br>
                            Accepted at <em>VISIGRAPP, 2022</em>
                            <br>
                              <a href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/Projects/ETL-face-task/ETL_VISSAP.pdf">Paper</a>
                              /
                              <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/etl-efficient-transfer-learning-for-face-tasks">Project Page</a>
                              /
                              <a href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/Projects/ETL-face-task/ETL-face-tast.mp4">Video</a>
                           <p>In this paper, we propose a technique that efficiently transfers a pre-trained model to a new task
                             by retaining only cross-task aware filters, resulting in a sparse transferred model. We demonstrate
                             the effectiveness of ETL by transferring VGGFace, a popular face recognition model to four diverse face tasks. </p>
                        </td>
                      </tr>



                <tr>
                  <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/dgaze.png" width="100%"></td>
                  <td width="100%" valign="center">
                      <papertitle>DGAZE: Driver Gaze Mapping on Road.</papertitle>
                    <br>
                    <strong>Isha Dua</strong>,
                     Thrupthi Ann John, Riya Gupta, C. V. Jawahar
                    <br>
                    <em>Accepted at IROS 2020 </em>
                    <br>
                              <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/dgaze">Project Page</a>
                              /
                              <a href="https://www.semanticscholar.org/paper/DGAZE%3A-Driver-Gaze-Mapping-on-Road-Dua-John/729e6eb649b04e1a403b54b5b5b4c73d77888905">Paper</a>
                              /
                              <a href="http://cvit.iiit.ac.in/images/Projects/DGAZE/DGAZE_dataset.zip">Dataset</a>
                    <p> DGAZE is a new dataset for mapping the driver's gaze onto the road. We collected 7 unique objects on road.
                       For each driver we collected 103 such objects. We provide both point and object annotations for driver gaze on road.
                    </p>
                    </td>
                 </tr>

                <tr>
                  <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/Autorate.png" width="100%"></td>
                  <td width="100%" valign="center">
                  <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="data/NAS.png" alt="blind-date" width="280" height="250"></td> -->
                  <!-- <td style="width:40%;vertical-align:middle"><img src="data/NAS.png" width="100%"></td> -->
                  <!-- <td width="75%" valign="middle"> -->
                  <!-- <td width="100%" height="100%"valign="center"> -->
                    <papertitle>Autorate: How attentive is the driver?.</papertitle>
                    <br>
                    <strong>Isha Dua</strong>, Akshay Uttama Nambi, Venkat Padmanabhn, C.V.Jawahar
                    <br>
                    <em>Accepted as <strong>Oral Paper</strong> in FG2019</em>
                    <br>
                              <a href="https://www.microsoft.com/en-us/research/uploads/prod/2019/05/AutoRate-FG2019.pdf">Paper</a>
                              <!-- /
                              <a href="data/MULA_11_CVPR21_ViQAR_slides.pdf">Slides</a>
                              /
                              <a href="data/MULA_11_CVPR21_ViQAR_video.mp4">Video</a>
                              /
                              <a href="data/MULA_11_CVPR21_ViQAR_poster.pdf">Poster</a> -->
                    <p> AutoRate is a system that leverages front camera of a windshield-mounted smartphone to
                      monitor driverâ€™s attention by combining several features.We derive a driver attention rating
                      by fusing spatio-temporal features based on the driver state and behavior such as he head pose,
                      eye gaze, eye closure, yawns, use of cellphones, etc.  </p>
                    </td>
                  </tr>



                <tr>
                  <td style="padding:10px;width:40%;max-width:40%;vertical-align:middle"><img src="data/DIA.png" width="100%"></td>
                  <td width="100%" valign="center">
                  <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="data/viqar_final.png" alt="blind-date" width="280" height="250"></td> -->
                <!-- <td style="width:40%;max-width:100%;vertical-align:middle"><img src="data/viqar_final.png" width="100%"></td> -->
                <!-- <td width="75%" valign="middle"> -->
                  <papertitle>Evaluation and visualization of driver inattention rating from facial features.</papertitle>
                  <br>
                  <strong>Isha Dua</strong>, Akshay Uttama Nambi, Venkat Padmanabhn, C.V.Jawahar
                  <br>
                  Accepted as Journal paper at <em>TBIOM, IEEE Biometrics</em> 2019
                  <br>
                            <a href="https://www.semanticscholar.org/paper/Evaluation-and-Visualization-of-Driver-Inattention-Dua-Nambi/4abce82add4ebe1bc43bca828edfb033095540f4">Paper</a>
                            <!-- /
                            <a href="data/MULA_11_CVPR21_ViQAR_slides.pdf">Slides</a>
                            /
                            <a href="data/MULA_11_CVPR21_ViQAR_video.mp4">Video</a>
                            /
                            <a href="data/MULA_11_CVPR21_ViQAR_poster.pdf">Poster</a> -->
                  <p> In this work, we propose soft attention mechanism in AutoRate which improves AutoRateâ€™s accuracy by 10%.
                    We use temporal and spatial attention to visualize the key frame and the key action which justify the modelâ€™s predicted rating.
                    </p>
                  </td>
                </tr>

              <tr>
                <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/human_elephant_collision.png" width="100%"></td>
                <td width="100%" valign="center">
                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="data/vayu.png" alt="blind-date" width="280" height="250"></td> -->
                <!-- <td style="width:40%;vertical-align:middle"><img src="data/vayu.png" width="100%"></td>
                <td width="100%" valign="center"> -->
                <!-- <td width="75%" valign="middle"> -->
                        <papertitle>A Computer Vision Framework for Detecting and Preventing Human-Elephant Collisions.</papertitle>
                        <br>
                        Pushkar Shukla, <strong>Isha Dua</strong>, Ankush Mittal, Balasubramanian Raman
                        <br>
                        Accepted at <em>ICCV, 2017</em>, Workshop
                        <br>
                          <a href="https://www.semanticscholar.org/paper/A-Computer-Vision-Framework-for-Detecting-and-Mittal Dua/21547c99f1c47a434a67f96fd5f772e90a0c4fef">Paper</a>
                          /
                          <a href="data/human_elephant_collision_poster.pdf">Poster</a>
                          /
                          <a href="data/human_elephant_collision_presentation.pdf">Presentation</a>
                       <p>The paper proposes a frame-work that relies on computer vision approaches for detecting and preventing Human Elephant Collision.
                          The technique initially recognizes the areas of conflict where accidents are most likely to occur.
                          This is followed by elephant detection system that identifies an elephant in the video frame which is then tracked
                          with respect to the area of conflict with a particle filter algorithm.</p>
                    </td>
                  </tr>


                  <tr>
                    <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/paper1.png" width="100%"></td>
                    <td width="100%" valign="center">
                    <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="data/vayu.png" alt="blind-date" width="280" height="250"></td> -->
                    <!-- <td style="width:40%;vertical-align:middle"><img src="data/vayu.png" width="100%"></td>
                    <td width="100%" valign="center"> -->
                    <!-- <td width="75%" valign="middle"> -->
                            <papertitle>A vision based human - elepahant collision detection system.</papertitle>
                            <br>
                            <strong>Isha Dua</strong>, Pushkar Shukla, Ankush Mittal
                            <br>
                            Accepted at <em>ICIIP, 2015</em>
                            <br>
                              <a href="https://ieeexplore.ieee.org/document/7414770/#full-text-section">Arxiv</a>
                              <!-- /
                              <a href="data/human_elephant_collision_poster.pdf">Poster</a>
                              /
                              <a href="data/human_elephant_collision_presentation.pdf">Presentation</a> -->
                           <p> The proposed paper seeks to identify elephants with the aid of a Video Camera.
                             The suggested methodology was applied to zones having high intervention of human beings and elephants.
                              Regions with higher human movements like roads were extracted from the initial video frames.
                              This process is followed by detecting motion in the video frame. The objects in the area of
                              motion are then identified as elephant or non-elephant with the help of PHOG features and
                              Support Vector Machines (SVM) classifiers..</p>
                        </td>
                      </tr>
                  </tbody></table>



                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      <tr>
                      <td style="padding:10px;width:100%;vertical-align:middle">
                        <heading>Experience</heading>
                      </td>
                    </tr>
                  </tbody></table>
                  <table cellpadding="20" border="0" width="100%" align="center"><tbody>
                    <tr>
                      <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/mbenz.png" width="60%"></td>
                      <td width="100%" valign="center">
                      <!-- <td style="padding:20px;width:20%;vertical-align:middle"><img src="data/Brown_University_Logo.png"></td>
                      <td width="75%" valign="center"> -->
                          <papertitle>Machine Learning Researcher, Mercedes Benz, R&D, India</papertitle>
                        <br><strong>Manager:</strong> <a href="https://www.linkedin.com/in/brijeshpillai/">Brijesh Pillai</a>
                          <p>
                          Currently working on domain adaptation for bridging sim to real gap. Previously worked on developing
                          accelerated deep learning framework for faster training.
                        </p>
                      </td>
                    </tr>

                      <tr>
                        <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/msr.png" width="70%"></td>
                        <td width="100%" valign="center">
                        <!-- <td style="padding:20px;width:20%;vertical-align:middle"><img src="data/iith.png"></td>
                        <td width="75%" valign="center"> -->
                            <papertitle>Microsoft Research, India</papertitle>
                          <br><strong>Supervisor:</strong> <a href="https://www.microsoft.com/en-us/research/people/padmanab/">Venkat Padmanabhan</a>
                            <p>
                              Summer Internship at Microsoft Research, India. Here I worked on driver attention rating on
                              Indian roads and mapping eye gaze to real world videos.
                          </p>
                        </td>
                      </tr>

                        <tr>
                        <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/talentsprint.jpg" width="60%"></td>
                        <td width="100%" valign="center">
                        <!-- <td style="padding:20px;width:20%;vertical-align:middle"><img src="data/iitd.jpeg"></td> -->
                        <!-- <td valign="top" width="68%"> -->
                        <!-- <td width="75%" valign="center"> -->
                            <papertitle>AIML Course</papertitle>
                            <br><strong>Mentor:</strong> <a href="https://faculty.iiit.ac.in/~jawahar/">Professor C.V.Jawahar</a>
                            and <a href="https://www.iiit.ac.in/people/faculty/anoop/">Professor Anoop M. Namboodiri </a>.
                            <br>
                            <p>
                              Teaching Assistant at AIML course conducted by <a href="https://faculty.iiit.ac.in/~jawahar/" target="_blank">Professor
                              C.V.Jawahar</a> and <a href="https://www.iiit.ac.in/people/faculty/anoop/" target="_blank">Professor Anoop M. Namboodiri
                              </a> in collaboration with <a href="https://www.talentsprint.com/" target="_blank">Talent Sprint</a> (Jan 2018-May 2018).
                            </p>
                        </td>
                      </tr>

                      <tr>
                      <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/rc.jpeg" width="60%"></td>
                      <td width="100%" valign="center">
                      <!-- <td style="padding:20px;width:20%;vertical-align:middle"><img src="data/iitd.jpeg"></td> -->
                      <!-- <td valign="top" width="68%"> -->
                      <!-- <td width="75%" valign="center"> -->
                          <papertitle>Raman Classes</papertitle>
                          <br><strong>Mentor:</strong> <a href="http://geu.ac.in/research/home.html">Professor Ankush Mittal </a>.
                          <br>
                          <p>
                            Teaching Assistant at Raman Classes(Feb 2016 - June 2016).
                            The course was taken by <a href="http://geu.ac.in/research/home.html" target="_blank">Professor Ankush Mittal.
                            </a> The course focus on teaching all computer science major courses.
                             </p>
                      </td>
                    </tr>
                    </tbody></table>





                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      <tr>
                      <td style="padding:10px;width:100%;vertical-align:middle">
                        <heading>Selected Projects</heading>
                      </td>
                    </tr>
                    </tbody></table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                  <!-- <table cellpadding="20" border="0" width="100%" align="center"><tbody> -->
                    <tr>
                      <!-- <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/viqar_proj.png" width="100%"></td> -->
                      <td width="100%" valign="center">
                      <papertitle>Eye Gaze Gaming</papertitle>
                      <br>
                      <i>supervised by </i> Professor C.V.Jawahar
                      <p> The project focuses on developing web camera based first person shooter game.
                         It provides advance gaming technology to play video games using head pose and eye gaze.
                         Also, extended the algorithm for browsing articles.
                      </p>
                      <p>
                      <a href="http://preon.iiit.ac.in/~isha_dua/research_webpage/weekly_progress/menu/EyeGazeGaming/video.html">[Demo video]</a>
                      /
                      <a href="http://preon.iiit.ac.in/~isha_dua/research_webpage/weekly_progress/menu/EyeGazeGaming/poster.html">[poster]</a>
                      /
                      <a href="https://docs.google.com/presentation/d/1dXdUI9LAWMOq9ODtP1zK1WBtOxEvkhIIMP0ueQNBUrs/edit#slide=id.gcb9a0b074_1_0">[presentation]</a>
                    </p></td>
                  </tr>

                  <!-- <tr> -->
                    <!-- <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/viqar_proj.png" width="100%"></td> -->
                    <!-- <td width="100%" valign="center"> -->
                          <!-- <td width="32%"><img src="data/iitd.jpeg" style="border-radius:15px" width="80%"></td> -->

                    <!-- <papertitle>Human-Elephant Collision Detection System using Particle Object Filtering</papertitle> -->
                    <!-- <br> -->
                    <!-- <i>supervised by </i> Professor Ankush Mittal -->
                    <!-- <p>The project use particle filtering algorithm to track elephants and prevent human elephant collision. -->
                    <!-- </p> -->
                    <!-- </td> -->
                    <!-- </tr> -->

                    <tr>
                      <!-- <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/viqar_proj.png" width="100%"></td> -->
                      <td width="100%" valign="center">
                            <!-- <td width="32%"><img src="data/iitd.jpeg" style="border-radius:15px" width="80%"></td> -->

                      <papertitle>Breathing Rate using camera</papertitle>
                      <br>
                      <i>work done at</i> India Hacks, 2017
                      <p>It is computer vision project in which using the camera which can be web camera or mobile camera,
                        the breathing rate of the person can be determined.
                      </p>
                      </td>
                    </tr>

                  <tr>
                      <!-- <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/viqar_proj.png" width="100%"></td> -->
                      <td width="100%" valign="center">
                            <!-- <td width="32%"><img src="data/iitd.jpeg" style="border-radius:15px" width="80%"></td> -->

                      <papertitle>As-Projective-As-Possible Image Stitching with Moving DLT</papertitle>
                      <br>
                      <i>supervised by </i> Professor Anoop Namboodiri
                      <p>The focus is on the task of image stitching by proposing as-projective-as-possible (APAP) warps,
                        i.e., warps that aim to be globally projective, yet allow local non-projective deviations to account for
                        violations to the assumed imaging conditions.
                      </p>
                      </td>
                  </tr>

                  <tr>
                      <!-- <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/viqar_proj.png" width="100%"></td> -->
                      <td width="100%" valign="center">
                            <!-- <td width="32%"><img src="data/iitd.jpeg" style="border-radius:15px" width="80%"></td> -->

                      <papertitle>Classification of book genre by cover and title:</papertitle>
                      <br>
                      <i>supervised by </i> Professor Avinash Sharma
                      <p> The project use particle filtering algorithm to track elephants and prevent human elephant collision. Using pattern recognization
                          approaches like Multi class-SVM , neural networks etc to distinguish books into different genre purely based on cover and
                          title without aprior knowledge of context,author or origin.
                      </p>
                      </td>
                  </tr>

                  <tr>
                      <!-- <td style="padding:20px;width:40%;max-width:40%;vertical-align:middle"><img src="data/viqar_proj.png" width="100%"></td> -->
                      <td width="100%" valign="center">
                            <!-- <td width="32%"><img src="data/iitd.jpeg" style="border-radius:15px" width="80%"></td> -->

                      <papertitle>Animation Effects using Image Morphing</papertitle>
                      <br>
                      <i>supervised by </i> Professor Vineet Gandhi
                      <p>Using Triangulation method to do morphing between human faces . The idea is to get a sequence of
                         intermediate images which when put together with the original images would represent the change from
                         one image to the other.
                      </p>
                      </td>
                  </tr>

          </tbody></table>

          <br>
          <p align="center">Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a></p>.
</html>
